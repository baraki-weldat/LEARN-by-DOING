{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Libraries \nI have installed locally the packages ","metadata":{}},{"cell_type":"code","source":"!pip install -q /kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl --no-deps","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T10:22:08.009524Z","iopub.execute_input":"2024-06-24T10:22:08.010644Z","iopub.status.idle":"2024-06-24T10:22:10.620553Z","shell.execute_reply.started":"2024-06-24T10:22:08.010584Z","shell.execute_reply":"2024-06-24T10:22:10.619096Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Requirement '/kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl'\n\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import important libraries ","metadata":{}},{"cell_type":"code","source":"# /kaggle/input/ai-mathematical-olympiad-prize/sample_submission.csv\n# /kaggle/input/ai-mathematical-olympiad-prize/AIMO Prize - Note on Language and Notation.pdf\n# /kaggle/input/ai-mathematical-olympiad-prize/train.csv\n# /kaggle/input/ai-mathematical-olympiad-prize/test.csv\n# /kaggle/input/ai-mathematical-olympiad-prize/aimo/competition.cpython-310-x86_64-linux-gnu.so\n# /kaggle/input/ai-mathematical-olympiad-prize/aimo/__init__.py\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Datasets ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-prize/train.csv\") \ntest = pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-prize/test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Trained Models Configuration","metadata":{}},{"cell_type":"code","source":"# Pretrained Model\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Loading the  model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n) \n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Defining LoRA parameters (Adapter Layer )\npeft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.05,\n    r=32,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n# Attach trainable adapters\nmodel = get_peft_model(model, peft_params)\n\n# Training Parameters\ntraining_params = TrainingArguments(\n    output_dir=\"./CodeLlama2Movies\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    warmup_steps=10,\n    optim=\"paged_adamw_32bit\",\n    logging_strategy=\"steps\",\n    save_steps=25,\n    logging_steps=1,\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"tensorboard\"\n)\n\n# Model fine-tuning\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_params,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_params,\n    packing= False,\n)\n\ntrainer.train()\n\n# Save the model\ntrainer.save_model(new_model)\ntokenizer.save_pretrained(new_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}